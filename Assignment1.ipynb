{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formalia:\n",
    "\n",
    "Please read the [assignment overview page](https://github.com/suneman/socialgraphs2025/wiki/Assignments) carefully before proceeding. This page contains information about formatting (including formats etc), group sizes, and many other aspects of handing in the assignment. \n",
    "\n",
    "_If you fail to follow these simple instructions, it will negatively impact your grade!_\n",
    "\n",
    "**Due date and time**: The assignment is due on Tuesday September 30th, 2025 at 23:55. Hand in your IPython notebook file (with extension `.ipynb`) via DTU Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.1: Exploring WS and BA models\n",
    "\n",
    "This first part draws on the Watts-Stogatz and Barabasi-Albert models from Week 3. You should provide solutions to the exercises with the following titles from **Part 1** \n",
    "\n",
    "* *Did you really read the text? Answer the following questions (no calculations needed) in your IPython notebook*\n",
    "\n",
    "* *WS edition*\n",
    "\n",
    "And from **Part 2**\n",
    "\n",
    "* *BA Edition*.\n",
    "  * **Note**: The second part of this exercise (after the questions to the text) first has you build a BA network step-by-step, but doesn't ask any questions. For that part, I would simply like you to write well-documented code that shows how you build the network. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1.2: Stats and visualization of the Rock Music Network\n",
    "\n",
    "This second part requires you to have built the network of Rock Musicians as described in the exercises for Week 4. You should complete the following exercise from **Part 2**.\n",
    "\n",
    "* *Explain your process in words*\n",
    "\n",
    "* *Simple network statistics and analysis*.\n",
    "\n",
    "  * **Note related to this and the following exercise**. It is nice to have the dataset underlying the statistics and visualization available when we grade. Therefore, I recommend that you create a small *network dataset*, which is simply your graph stored in some format that you like (since it's only a few hundred nodes and a few thousand edges, it won't take up a lot of space). You can then place that network one of your group members' GitHub account (or some other server that's available online) and have your Jupyter Notebook fetch that dataset when it runs. (It's OK to use an LLM for help with setting this up, if it seems difficult). \n",
    "\n",
    "And the following exercise from **Part 3**\n",
    "\n",
    "* *Let's build a simple visualization of the network*\n",
    "\n",
    "And that's it! You're all set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 PART 2 - Explain process in words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we did was to trying to understand how Wikipedia stored links inside pages and we played a bit for understanding how to extract single performer names from the List of mainstream rock performers provided using RegEx.\n",
    "\n",
    "When we understood how to do it, with a function we extracted all the names of the page, ending up with 490 titles, with also a sanity check for veryfing that any other title (for examples reference links) ended up in our list.\n",
    "\n",
    "Then we have set up a function that, for every performer title found, call the endpoint of Wikipedia API in order to get the cleanest source text for each performer's page (link-parking would have been easier).\n",
    "We downloaded as wikitext since keeps internal links perfectly for then parsing with RegEx and saved as one .txt per title.\n",
    "\n",
    "Then, at this point, we had all the elements needed for building the network: each performer bacame a not and every performer page has been parsed with a regex to extract outgoing kinks; if a link pointed to another performer in my set we added a directed edge A->B. Then we computed a word count from the cleaned wikitext and stored it as a content_length attribute on each node. This can be used as a metric for how much information exists about a performer and and we hoped then to can correlate this to the number of outgoing/ingoing edges.\n",
    "\n",
    "Finally we removed isolated nodes (no in- or out-links), then kept the largest weakly connected component (LWCC) for analysis.\n",
    "\n",
    "The hardest part was to correctly extract and filter the links. Wikipedia pages are full of internal references, so it was still necessary to ignore links belonging to other namespaces such as Category: or File:, and to double-check that the links we kept really matched one of our performers. \n",
    "We used LLM mostly as a way to create the pipeline and to test whether regexes and API calls were set up in a robust way. Once I had these suggestions, I always verified them manually on a few performer pages, adjusted when needed, and only then scaled up the process.\n",
    "\n",
    "We don't really know what we would change if we would need to do it again, maybe inspecting other types of parser which would make link extraction and text cleaning more robust. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2 - Simple network statistics and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
